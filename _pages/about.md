---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<h2 id="about-me">About Me</h2>

Hi! I‚Äôm **Shuowen Li (ÊùéÁ°ïÊñá)**, a second-year M.S. student in Optical Engineering at **Tsinghua University**, advised by Prof. Liangcai Cao. I also work as a research intern at the **Academy of Arts & Design**, mentored by Prof. Haipeng Mi. Previously, I received my B.S. in Physics from Lanzhou University, where I studied nanomaterial design under Prof. Hao Jia.

My research focuses on **human‚ÄìAI interaction**, **computational imaging**, and **creative robotics**. I am interested in how intelligent systems can support human creativity by enabling semantic evolution, co-creative storytelling, and generative design. Combining science and art, I build interactive systems that help people create with AI in both digital and physical contexts.

Check out my [CV](../assets/CV_Shuowen_Li.pdf) for more!




## üìÑ Publications {#publications}

### Peer-reviewed Conference Papers

(\* indicates co-first author)

1. **Shihan Qiu\*, Yuhan Xie\*, Shuowen Li\***, Wei Guo, Xiaoyue Gao, Yijie Guo.
   *Embo: A Wearable Robot Transforming Child-Directed Verbal Aggression into Tactile Feedback*.
   In: *ACM/IEEE International Conference on Human-Robot Interaction (HRI '24) Companion*, 2024, pp. 857‚Äì861.

2. **Shuowen Li\*, Kexin Wang, Minglu Fang, Danqi Huang, Ali Asadipour, Haipeng Mi, Yitong Sun**.
   *Participatory Evolution of Artificial Life Systems via Semantic Feedback*.
   Submitted to *SIGGRAPH Asia 2025 Art Papers*. (*Under Review*)

3. **Ruhan Wang, Shuowen Li, Peiran Zhang, Danqi Huang, Yijie Guo, Haipeng Mi**.
   *PuppetLine: An Interactive System for Embodied Storytelling with LLM-driven Swarm Robots*.
   Submitted to *TEI 2026*. (*Under Review*)



### Journal Papers

4. **Shuowen Li, Liangcai Cao**.
   *Multidimensional Crosstalk Analysis in Autostereoscopic Displays: Integrating Subjective and Objective Evaluations for Image Quality Assessment*.
   *Optics Express*, 33(8), 2025, pp. 16911‚Äì16924.

5. **Shuowen Li, Yunhui Gao, Jiachen Wu, Mingjie Wang, Zhangcheng Huang, Shumei Chen, Liangcai Cao**.
   *Lensless Camera: Unraveling the Breakthroughs and Prospects*.
   *Fundamental Research*, 2024. (*In Press*)

6. **Shuowen Li, Yunhui Gao, Jiachen Wu, Liangcai Cao**.
   *Blind Deblurring of Astronomical Images Using SCGTV-Based Single-Frame Method*.
   *Optics Express*, 32(20), 2024, pp. 35579‚Äì35593.




### üìò Book Translation

<div style="display: flex; align-items: flex-start; gap: 16px; margin-top: 10px;">

  <img src="/images/matter_cover.jpg" alt="Matter book cover" style="width: 120px; height: auto; border: 1px solid #ccc;">

  <div>
    <p><em>Geoff Cottrell</em>. <strong><em>Matter: A Very Short Introduction</em></strong>.<br>
    Translated by Xiang Liu, <strong>Shuowen Li</strong>, Jiageng Li.<br>
    Yilin Press, 2024. ISBN: 9787575301671.</p>
    <p><em>A concise yet wide-ranging introduction to matter, guiding readers through its forms from fundamental particles to cosmic structures‚Äîwith clarity and accessibility.</em></p>
  </div>

</div>



## üß™ Projects {#projects}

### üß¨ <strong>Semantic-Guided Artificial Life System</strong>

<div style="display: flex; align-items: flex-start; gap: 16px; margin-top: 10px;">

  <video src="/videos/semantic_life.mp4" autoplay loop muted playsinline style="width: 220px; border-radius: 8px; border: 1px solid #ccc;"></video>

  <div>
    <p><em>Academy of Arts & Design, Tsinghua University (2023‚Äìpresent)</em></p>
    <p>Developed a real-time interactive system where users evolve digital lifeforms using <strong>natural language prompts</strong>. Integrated <strong>CLIP-based multimodal evaluation</strong> and <strong>CMA-ES optimization</strong>, supporting both semantic guidance and emergent swarm behaviors. Enables participatory generative design in the spirit of ‚Äúevolving with intention‚Äù.</p>
  </div>

</div>


### ü§ñ **PuppetLine: Swarm Robotic Storytelling**

*Academy of Arts & Design, Tsinghua University (2024‚Äìpresent)*

Built a tangible storytelling system using **Toio robots** and **large language models (LLMs)**. Translates children‚Äôs narrative inputs into synchronized multi-robot actions and emotions. The system empowers **co-creative expression** through embodied interaction.

### ‚úã **Embo: A Wearable Robot for Empathy Education**

*Academy of Arts & Design, Tsinghua University (2023)*

Designed a wearable puppet robot that transforms **verbal aggression** into **tactile pressure feedback**, addressing bullying scenarios in child‚Äìrobot interaction. Combines **natural language analysis** and **haptic feedback** to build emotional awareness.

### üåå **Astronomical Image Reconstruction**

*Dept. of Precision Instrument, Tsinghua University (2022‚Äì2025)*

Developed a GPU-accelerated pipeline for **turbulence removal** in single-frame astronomical images using **blind deconvolution**, **deformable convolution**, and **diffusion models**. Collaborated on hardware‚Äìsoftware co-design with a **four-aperture telescope array**.

### üñºÔ∏è **3D Display Quality Assessment**

*Dept. of Precision Instrument, Tsinghua University (2022‚Äì2024)*

Proposed a hybrid evaluation framework for **glasses-free 3D displays**, integrating **optical metrics** and **human visual comfort** analysis. Contributed to drafting **industry standards** for autostereoscopic display technologies.




## üéØ Research Interests {#interests}

* **Human‚ÄìAI Co-Creation**: Designing interactive systems that fuse generative AI with human intention, language, and emotion.
* **Computational Design Tools**: Building tools for evolving, simulating, and editing digital forms‚Äîacross visual, spatial, and behavioral dimensions.
* **Creative Robotics & Embodied Interaction**: Leveraging swarm behavior, wearable tech, and storytelling to create expressive physical interactions.
* **Computational Imaging**: From lensless cameras to astronomical deblurring, exploring inverse problems in optical system design.
* **Visual Perception & Evaluation**: Studying how humans perceive 3D content, depth, and image quality, and modeling this in computational terms.

